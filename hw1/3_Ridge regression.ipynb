{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3(a) [10 points] Write a function ridge regression(tX; tY; l) that takes the training features, training responses and regularizing parameter $\\lambda$, and outputs the exact solution $\\theta$ for ridge regression. Report the resulting value of $\\theta$ for $\\lambda$ = 0.15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     y\n",
      "0    -1.13933054418729\n",
      "1    -1.38956280272455\n",
      "2    -1.46427864519255\n",
      "3   -0.771402282836843\n",
      "4    -2.45451924323433\n",
      "5    -2.53173136529269\n",
      "6   -0.585147032511934\n",
      "7    -4.53260862043966\n",
      "8   -0.593824236717293\n",
      "9    -1.38688905971912\n",
      "10   -2.49527766850141\n",
      "11   -1.73538943211652\n",
      "12    0.91300105348625\n",
      "13   -4.14696407077148\n",
      "14   -3.47921987976619\n",
      "15   -2.00766789387212\n",
      "16   -3.72876356921769\n",
      "17   -1.11373904691801\n",
      "18  -0.716923558501225\n",
      "19   -1.33254433207279\n",
      "20  -0.984736485140916\n",
      "21   -4.77208525334549\n",
      "22  -0.989487677263845\n",
      "23   0.444727886065754\n",
      "24  -0.129070694759691\n",
      "25   -2.77946763634608\n",
      "26    1.20221693265651\n",
      "27   -3.91963951568596\n",
      "28   -4.37192529305329\n",
      "29  -0.498441678852203\n",
      "30   -4.59804438953968\n",
      "31  -0.382168049862368\n",
      "32   -1.56516376434081\n",
      "33  -0.720897800318107\n",
      "34  -0.724296802078357\n",
      "35    -1.8829537957885\n",
      "36  -0.237402764116535\n",
      "37  -0.714888106720234\n",
      "38   -3.64062349641719\n",
      "39   -3.01168856716308\n",
      "40  -0.477597120292055\n",
      "41   -3.12712799176494\n",
      "42   -4.75174196842826\n",
      "43  -0.694166968747554\n",
      "44   -1.65252122013147\n",
      "45  -0.098364270780666\n",
      "46  -0.873817872518156\n",
      "47  -0.012873342457726\n",
      "48   -0.06954224996399\n",
      "49   -1.70276013102121\n",
      "The validation x is: \n",
      "[[-1.47752373 -0.0502532  -0.17023633  1.        ]\n",
      " [ 0.90709037  0.66451566  0.47865149  1.        ]\n",
      " [ 0.4003217   0.43267376 -0.43504849  1.        ]\n",
      " [-1.65365073  0.126796    1.00236757  1.        ]\n",
      " [-1.06691275 -0.83842795 -0.34888078  1.        ]\n",
      " [-0.68878191 -0.80913707  0.43642513  1.        ]\n",
      " [ 0.35745996  1.13429946  1.56657725  1.        ]\n",
      " [ 0.09528788 -2.06401766  1.04123572  1.        ]\n",
      " [ 1.23035184  1.61568621  1.48715984  1.        ]\n",
      " [-0.83620287  0.07849988  2.13596839  1.        ]]\n",
      "The test x is: \n",
      "[[-0.43319969 -0.63613778 -0.38595494  1.        ]\n",
      " [ 0.28831115  0.24175838 -0.16395618  1.        ]\n",
      " [-2.13894535  0.97237413  0.10424867  1.        ]\n",
      " [ 0.99927052 -1.11433758 -0.45458174  1.        ]\n",
      " [ 0.49052428 -0.92271326 -0.35495995  1.        ]\n",
      " [ 0.38507689  0.08352817  0.02790376  1.        ]\n",
      " [ 0.69112273 -1.00672565 -0.88943049  1.        ]\n",
      " [-0.41774901  0.40816784  1.80858458  1.        ]\n",
      " [ 0.49400495  1.02792757 -1.00655099  1.        ]\n",
      " [ 0.58197659  0.63871748 -1.42460059  1.        ]\n",
      " [-0.01784407  0.71766039 -0.61215247  1.        ]\n",
      " [ 0.87391878 -1.75733762  0.4195343   1.        ]\n",
      " [-0.66181284  0.49298387 -0.04672385  1.        ]\n",
      " [ 1.23736049  2.21406654  1.58620271  1.        ]\n",
      " [-1.0553783   0.8954994  -1.84451136  1.        ]\n",
      " [ 2.30714568  0.25333497 -0.48961114  1.        ]\n",
      " [-0.4755072   2.056542    1.57110362  1.        ]\n",
      " [ 1.41596994 -0.92738418 -0.05858269  1.        ]\n",
      " [ 1.62513334 -1.12246556 -1.09691528  1.        ]\n",
      " [ 0.40780549  1.02729963 -0.04277794  1.        ]\n",
      " [ 1.06301487 -1.61341439  0.45159474  1.        ]\n",
      " [-0.67374261  0.74646502  0.88104198  1.        ]\n",
      " [ 0.13187129  0.31486386  1.09636616  1.        ]\n",
      " [-0.21322547  0.74141203  0.80339276  1.        ]\n",
      " [ 1.05339677  1.225563    0.16642039  1.        ]\n",
      " [ 0.15524151  0.10448045 -0.52662439  1.        ]\n",
      " [-0.69002679  0.84764888 -0.15939914  1.        ]\n",
      " [-0.45254444  0.74638293 -0.14693742  1.        ]\n",
      " [ 0.99099792 -0.99198341  0.56108373  1.        ]\n",
      " [ 0.53267167 -0.61727065  0.5028914   1.        ]\n",
      " [-1.94765626  0.15310133  0.55662237  1.        ]\n",
      " [ 1.46155517 -0.28306337  1.09531722  1.        ]\n",
      " [ 0.54346479 -1.82947935 -0.0865562   1.        ]\n",
      " [-0.31688096  0.67619353  0.30241958  1.        ]\n",
      " [-0.79405799 -0.02982322 -1.87777205  1.        ]\n",
      " [ 0.27861628  1.48764306 -1.04769245  1.        ]\n",
      " [ 0.61099245  1.00413207  0.56495587  1.        ]\n",
      " [ 0.06320248  1.48185744  0.07624095  1.        ]\n",
      " [-0.97676235  0.85265757 -0.78701903  1.        ]\n",
      " [-1.93031271 -0.60963352  0.77863057  1.        ]]\n",
      "The validation y is: \n",
      "[[-1.13933054]\n",
      " [-1.3895628 ]\n",
      " [-1.46427865]\n",
      " [-0.77140228]\n",
      " [-2.45451924]\n",
      " [-2.53173137]\n",
      " [-0.58514703]\n",
      " [-4.53260862]\n",
      " [-0.59382424]\n",
      " [-1.38688906]]\n",
      "The test y is: \n",
      "[[-2.49527767]\n",
      " [-1.73538943]\n",
      " [ 0.91300105]\n",
      " [-4.14696407]\n",
      " [-3.47921988]\n",
      " [-2.00766789]\n",
      " [-3.72876357]\n",
      " [-1.11373905]\n",
      " [-0.71692356]\n",
      " [-1.33254433]\n",
      " [-0.98473649]\n",
      " [-4.77208525]\n",
      " [-0.98948768]\n",
      " [ 0.44472789]\n",
      " [-0.12907069]\n",
      " [-2.77946764]\n",
      " [ 1.20221693]\n",
      " [-3.91963952]\n",
      " [-4.37192529]\n",
      " [-0.49844168]\n",
      " [-4.59804439]\n",
      " [-0.38216805]\n",
      " [-1.56516376]\n",
      " [-0.7208978 ]\n",
      " [-0.7242968 ]\n",
      " [-1.8829538 ]\n",
      " [-0.23740276]\n",
      " [-0.71488811]\n",
      " [-3.6406235 ]\n",
      " [-3.01168857]\n",
      " [-0.47759712]\n",
      " [-3.12712799]\n",
      " [-4.75174197]\n",
      " [-0.69416697]\n",
      " [-1.65252122]\n",
      " [-0.09836427]\n",
      " [-0.87381787]\n",
      " [-0.01287334]\n",
      " [-0.06954225]\n",
      " [-1.70276013]]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "with open('data/3/hw1_ridge_x.dat', 'r') as f1:\n",
    "    # next(f) # skip first row\n",
    "    df_x = pd.DataFrame((l.rstrip().split(',') for l in f1), columns=['x1','x2','x3','x4'])\n",
    "\n",
    "with open('data/3/hw1_ridge_y.dat', 'r') as f2:\n",
    "    # next(f) # skip first row\n",
    "    #df_y = pd.DataFrame()\n",
    "    #label_list = [l.rstrip().split()[0] for l in f2]\n",
    "    #print(label_list)\n",
    "    df_y = pd.DataFrame((l.rstrip().split() for l in f2), columns=['y'])\n",
    "    #df_y['y'] = label_list\n",
    "    print(df_y)\n",
    "df_x=df_x.astype(float)\n",
    "df_y=df_y.astype(float)\n",
    "\n",
    "vX = df_x.head(10).values\n",
    "tX = df_x.tail(40).values\n",
    "vY = np.array(df_y.head(10))\n",
    "tY = np.array(df_y.tail(40))\n",
    "print('The validation x is: \\n{}'.format(vX))\n",
    "print('The test x is: \\n{}'.format(tX))\n",
    "print('The validation y is: \\n{}'.format(vY))\n",
    "print('The test y is: \\n{}'.format(tY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.53590673],\n",
       "       [ 1.20293332],\n",
       "       [ 0.04334568],\n",
       "       [-1.85492455]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ridgeregression(tX, tY, l):\n",
    "    first_dimension = tX.shape[0]\n",
    "    second_dimension = tX.shape[1]\n",
    "    identity_matrix = np.identity(second_dimension)\n",
    "    identity_matrix[3][3] = 0\n",
    "    n_lambda_i = first_dimension * l * identity_matrix\n",
    "    xt_x = np.transpose(tX).dot(tX)\n",
    "    xt_y = np.transpose(tX).dot(tY)\n",
    "    add_matrix = n_lambda_i + xt_x\n",
    "    inverse_matrix = np.linalg.inv(add_matrix)\n",
    "    weight = inverse_matrix.dot(xt_y)\n",
    "    return weight\n",
    "ridgeregression(tX, tY,0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3(b) [10 points] Use the following code to plot graphs of the validation loss and training loss as $\\lambda$ varies on logarithmic scale from $\\lambda$ = 10<sup>-5</sup> to $\\lambda$ = 10<sup>0</sup>. Write down the value of $\\lambda$ that minimizes the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches \n",
    "tn = tX.shape[0]\n",
    "vn = vX.shape[0]\n",
    "tloss = []\n",
    "vloss = []\n",
    "index = -np.arange(0, 5, 0.1)\n",
    "lowest_index = 0.0\n",
    "lowest_validation_loss = 100.0\n",
    "for i in index:\n",
    "    w = ridgeregression(tX, tY, 10 ** i )\n",
    "    if i == 0.0:\n",
    "        lowest_index = i\n",
    "        lowest_validation_loss = np.sum((np.dot(vX, w) - vY) ** 2 )/vn/2\n",
    "    \n",
    "    tloss = tloss + [np.sum((np.dot(tX, w) - tY) ** 2 )/tn/2]\n",
    "    vloss = vloss + [np.sum((np.dot(vX, w) - vY) ** 2 )/vn/2]\n",
    "    current_validation_loss = np.sum((np.dot(vX, w) - vY) ** 2 )/vn/2\n",
    "    if current_validation_loss < lowest_validation_loss:\n",
    "        lowest_validation_loss = current_validation_loss\n",
    "        lowest_index = i\n",
    "plt.plot(index,np.log(tloss),'r')\n",
    "plt.plot(index,np.log(vloss), 'b')\n",
    "blue_patch = mpatches.Patch(color='blue', label='Validation Loss')\n",
    "red_patch = mpatches.Patch(color='red', label='Training Loss')\n",
    "plt.legend(handles=[blue_patch, red_patch])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The power to the lambda that minimizes the validation loss is -1.1, \n",
      "with a validation loss of 0.0042636472365700255\n"
     ]
    }
   ],
   "source": [
    "print('''The power to the lambda that minimizes the validation loss is {}, \n",
    "with a validation loss of {}'''.format(lowest_index.round(3), lowest_validation_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The value of $\\lambda$ that minmizes the validation loss is $\\lambda$= 10<sup>-1.9</sup>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
